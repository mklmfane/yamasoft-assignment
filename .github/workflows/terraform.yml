name: Terraform

on:
  workflow_dispatch:
    inputs:
      do_apply:
        description: "Run full terraform apply after bootstrap"
        required: true
        default: "true"
        type: choice
        options: ["true","false"]
      do_destroy:
        description: "Run terraform destroy (manual only)"
        required: true
        default: "true"
        type: choice
        options: ["true","false"]
  push:
    branches: ["master", "main"]
  pull_request:
    branches: ["*"]

permissions:
  id-token: write
  contents: read

env:
  TF_IN_AUTOMATION: "true"
  AWS_REGION: eu-west-1

jobs:
  bootstrap:
    name: Bootstrap (state bucket, IAM policies, OIDC)
    runs-on: ubuntu-latest
    if: ${{ github.event_name != 'pull_request' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Guard: we need long-lived AWS keys just for bootstrap
      - name: Require AWS secrets for bootstrap
        env:
          HAS_AKID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          HAS_SAK:  ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          set -euo pipefail
          if [ -z "${HAS_AKID}" ] || [ -z "${HAS_SAK}" ]; then
            echo "ERROR: Missing repo secrets AWS_ACCESS_KEY_ID and/or AWS_SECRET_ACCESS_KEY." >&2
            exit 1
          fi

      - name: Configure AWS credentials (bootstrap via access keys)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}

      - name: Verify caller identity
        run: aws sts get-caller-identity

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.3
          terraform_wrapper: false

      # Temporarily remove any "backend \"s3\"" blocks so we can init locally
      - name: Temporarily disable S3 backend files
        run: |
          set -euo pipefail
          mkdir -p .ci-backend-stash
          mapfile -t files < <(grep -Rl --include='*.tf' --include='*.tf.json' 'backend *"s3"' . | grep -v '^./\.ci-backend-stash/' || true)
          if [ ${#files[@]} -eq 0 ]; then
            echo "No backend \"s3\" blocks found to stash."
            exit 0
          fi
          printf '%s\n' "${files[@]}" > .ci-backend-stash/manifest.txt
          i=0
          for f in "${files[@]}"; do
            echo "Stashing $f"
            cp "$f" ".ci-backend-stash/file_${i}"
            rm "$f"
            i=$((i+1))
          done

      - name: Terraform init (local backend)
        run: terraform init -reconfigure -input=false

      # Avoid duplicate IAM policies: detect existing ARNs and pass as TF vars
      - name: Detect existing IAM policy ARNs
        shell: bash
        run: |
          set -euo pipefail
          get_arn () {
            local name="$1"
            local arn
            arn=$(aws iam list-policies --scope Local \
                    --query "Policies[?PolicyName=='${name}'].Arn | [0]" \
                    --output text 2>/dev/null || echo "")
            if [ "$arn" = "None" ] || [ "$arn" = "null" ] || [ -z "$arn" ]; then
              echo ""
            else
              echo "$arn"
            fi
          }
          B_ACK="$(get_arn tf-backend-rw)"
          VPC_APPLY="$(get_arn tf-vpc-apply)"

          echo "TF_VAR_existing_backend_rw_policy_arn=$B_ACK"     >> "$GITHUB_ENV"
          echo "TF_VAR_existing_vpc_apply_policy_arn=$VPC_APPLY"  >> "$GITHUB_ENV"

          echo "Detected tf-backend-rw: ${B_ACK:-<none>}"
          echo "Detected tf-vpc-apply : ${VPC_APPLY:-<none>}"

          
    # avoid duplicated github oidc iam role
      - name: Dectect existing github oidc iam role
        shell: bash
        run: |
          set -euo pipefail

          # --- Detect existing GitHub OIDC provider by URL (safe with pipefail) ---
          # Collect ARNs first (no failing pipeline), then loop.
          PROV_ARN=""
          ARNS=$(aws iam list-open-id-connect-providers \
              --query 'OpenIDConnectProviderList[].Arn' \
              --output text 2>/dev/null || true)

          # Iterate ARNs (space or tab separated). No pipelines here, so pipefail won't bite.
          for A in $ARNS; do
            URL=$(aws iam get-open-id-connect-provider \
               --open-id-connect-provider-arn "$A" \
               --query 'Url' --output text 2>/dev/null || true)
            if [ "$URL" = "https://token.actions.githubusercontent.com" ]; then
              PROV_ARN="$A"
              break
            fi
          done

          # --- Detect existing role (don't fail if it's missing) ---
          ROLE_ARN=$(aws iam get-role --role-name github-oidc-provider-aws \
              --query 'Role.Arn' --output text 2>/dev/null || true)
          # Normalize AWS CLI "None"/"null"
          [ "$ROLE_ARN" = "None" ] && ROLE_ARN=""
          [ "$ROLE_ARN" = "null" ] && ROLE_ARN=""

          # Export for Terraform
          echo "TF_VAR_existing_oidc_provider_arn=${PROV_ARN}" >> "$GITHUB_ENV"
          echo "TF_VAR_existing_role_arn=${ROLE_ARN}"         >> "$GITHUB_ENV"

          # Optionally prevent creation in the module
          if [ -n "$PROV_ARN" ]; then echo "TF_VAR_create_oidc_provider=false" >> "$GITHUB_ENV"; fi
          if [ -n "$ROLE_ARN" ]; then echo "TF_VAR_create_oidc_role=false"     >> "$GITHUB_ENV"; fi

          echo "Provider: ${PROV_ARN:-<none>}  Role: ${ROLE_ARN:-<none>}"

      # Bootstrap core infra (targets on purpose, to create backend + OIDC)
      - name: Terraform apply (bootstrap modules only)
        run: |
          set -euo pipefail
          terraform apply \
            -input=false \
            -auto-approve \
            -target=module.s3_bucket_state_oidc \
            -target=module.iam_tf_policies \
            -target=module.github_oidc

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Export bootstrap outputs
        id: out
        run: |
          set -euo pipefail
          tfjson=$(terraform output -json || echo '{}')

          # read the correct keys (adjust these three lines to your actual outputs)
          BUCKET=$(echo "$tfjson" | jq -r '.s3_bucket_id.value // empty')
          KEY=$(echo "$tfjson" | jq -r '.backend_key.value // empty')
          USE_LOCKFILE=$(echo "$tfjson" | jq -r '.use_lockfile.value // "true"')

          # ðŸ‘‡ use the actual module output name; earlier we defined "effective_role_arn"
          ROLE_ARN=$(echo "$tfjson" | jq -r '.effective_role_arn.value // empty')

          # fallback: if TF output was empty, detect by name (non-fatal)
          if [ -z "$ROLE_ARN" ]; then
            ROLE_ARN=$(aws iam get-role \
              --role-name github-oidc-provider-aws \
              --query 'Role.Arn' --output text 2>/dev/null || true)
              [ "$ROLE_ARN" = "None" ] && ROLE_ARN=""
              [ "$ROLE_ARN" = "null" ] && ROLE_ARN=""
          fi

          echo "bucket=$BUCKET"             >> $GITHUB_OUTPUT
          echo "key=$KEY"                   >> $GITHUB_OUTPUT
          echo "use_lockfile=$USE_LOCKFILE" >> $GITHUB_OUTPUT
          echo "role_arn=$ROLE_ARN"         >> $GITHUB_OUTPUT
          echo "Resolved role_arn: ${ROLE_ARN:-<empty>}"

      - name: Restore S3 backend files
        run: |
          set -euo pipefail
          if [ ! -f .ci-backend-stash/manifest.txt ]; then
            echo "Nothing to restore."
            exit 0
          fi
          i=0
          while IFS= read -r dest; do
            echo "Restoring to $dest"
            mkdir -p "$(dirname "$dest")"
            mv ".ci-backend-stash/file_${i}" "$dest"
            i=$((i+1))
          done < .ci-backend-stash/manifest.txt
          rm -rf .ci-backend-stash
      
      - name: Debug TF outputs
        run: |
          set -euo pipefail
          terraform output

    outputs:
      bucket:       ${{ steps.out.outputs.bucket }}
      key:          ${{ steps.out.outputs.key }}
      use_lockfile: ${{ steps.out.outputs.use_lockfile }}
      role_arn:     ${{ steps.out.outputs.role_arn }}
    

  deploy:
    name: Backend migrate âžœ Plan / Apply (OIDC)
    runs-on: ubuntu-latest
    needs: bootstrap

    if: ${{ needs.bootstrap.result == 'success' && needs.bootstrap.outputs.role_arn != '' && needs.bootstrap.outputs.bucket != '' && needs.bootstrap.outputs.key != '' }}

    env:
      TF_BACKEND_BUCKET:       ${{ needs.bootstrap.outputs.bucket }}
      TF_BACKEND_KEY:          ${{ needs.bootstrap.outputs.key }}
      TF_BACKEND_USE_LOCKFILE: ${{ needs.bootstrap.outputs.use_lockfile }}
      AWS_OIDC_ROLE_ARN:       ${{ needs.bootstrap.outputs.role_arn }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ env.AWS_OIDC_ROLE_ARN }}
          role-session-name: gha-terraform

      - name: Verify caller identity (OIDC)
        run: aws sts get-caller-identity

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.3
          terraform_wrapper: false

      - name: Cache .terraform providers
        uses: actions/cache@v4
        with:
          path: |
            **/.terraform
            ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-tf-${{ hashFiles('**/.terraform.lock.hcl') }}
          restore-keys: |
            ${{ runner.os }}-tf-

      - name: Echo role ARN for aws oidc github role
        run: |
          echo "Assuming role: $AWS_OIDC_ROLE_ARN"           
      
      - name: Terraform fmt (non-blocking)
        run: terraform fmt -recursive -diff || true

      - name: Terraform init (S3 backend, migrate state)
        shell: bash
        run: |
          set -euo pipefail
          ARGS=(-migrate-state -input=false)

          if [ -n "${TF_BACKEND_BUCKET:-}" ]; then
            ARGS+=(-backend-config="bucket=${TF_BACKEND_BUCKET}")
          fi
          if [ -n "${TF_BACKEND_KEY:-}" ]; then
            ARGS+=(-backend-config="key=${TF_BACKEND_KEY}")
          fi
          if [ -n "${AWS_REGION:-}" ]; then
            ARGS+=(-backend-config="region=${AWS_REGION}")
          fi
          # Optional: only include dynamodb_table if you actually use it
          if [ -n "${TF_BACKEND_DDB_TABLE:-}" ]; then
            ARGS+=(-backend-config="dynamodb_table=${TF_BACKEND_DDB_TABLE}")
          fi
          # Do NOT pass use_lockfile; it isn't an S3 backend setting and if empty it breaks init
          terraform init "${ARGS[@]}"        

      - name: Terraform validate
        run: terraform validate -no-color

      # PRs: plan only
      - name: Terraform plan (PR)
        if: github.event_name == 'pull_request'
        run: terraform plan -input=false -no-color -out=tfplan.bin

      - name: Upload plan (PR)
        if: github.event_name == 'pull_request'
        uses: actions/upload-artifact@v4
        with:
          name: tfplan
          path: tfplan.bin
          retention-days: 3

      # Non-PRs: plan & apply
      - name: Terraform plan (apply path)
        if: github.event_name != 'pull_request'
        run: terraform plan -input=false -no-color

      - name: Terraform apply
        if: github.event_name != 'pull_request' && (github.event_name != 'workflow_dispatch' || inputs.do_apply == 'true')
        run: terraform apply -input=false -auto-approve

      # Manual destroy only
      - name: Terraform destroy (manual only)
        if: github.event_name == 'workflow_dispatch' && inputs.do_destroy == 'true'
        run: terraform destroy -input=false -auto-approve
