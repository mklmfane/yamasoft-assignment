name: Terraform

on:
  workflow_dispatch:
    inputs:
      do_apply:
        description: "Run full terraform apply after bootstrap"
        required: true
        default: "true"
        type: choice
        options: ["true","false"]
      do_destroy:
        description: "Run terraform destroy (manual only)"
        required: true
        default: "true"
        type: choice
        options: ["true","false"]
  push:
    branches: ["master", "main"]
  pull_request:
    branches: ["*"]

permissions:
  id-token: write
  contents: read

env:
  TF_IN_AUTOMATION: "true"
  AWS_REGION: eu-west-1
  TF_VAR_bucket_prefix_name: tf-state-s3-bucket
  TF_VAR_lock_table: tf-state-lock

jobs:
  bootstrap:
    name: Bootstrap (state bucket, IAM policies, OIDC)
    runs-on: ubuntu-latest
    if: ${{ github.event_name != 'pull_request' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Use long-lived keys ONLY for bootstrap
      - name: Require AWS secrets for bootstrap
        env:
          HAS_AKID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          HAS_SAK:  ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          set -euo pipefail
          if [ -z "${HAS_AKID}" ] || [ -z "${HAS_SAK}" ]; then
            echo "ERROR: Missing repo secrets AWS_ACCESS_KEY_ID and/or AWS_SECRET_ACCESS_KEY." >&2
            exit 1
          fi

      - name: Configure AWS credentials (bootstrap via access keys)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}

      - name: Verify caller identity
        run: aws sts get-caller-identity

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.3
          terraform_wrapper: false

      # Temporarily remove any S3 backend blocks so we can init locally
      - name: Temporarily disable S3 backend files
        run: |
          set -euo pipefail
          mkdir -p .ci-backend-stash
          mapfile -t files < <(grep -Rl --include='*.tf' --include='*.tf.json' 'backend *"s3"' . | grep -v '^./\.ci-backend-stash/' || true)
          if [ ${#files[@]} -eq 0 ]; then
            echo "No backend \"s3\" blocks found to stash."
            exit 0
          fi
          printf '%s\n' "${files[@]}" > .ci-backend-stash/manifest.txt
          i=0
          for f in "${files[@]}"; do
            cp "$f" ".ci-backend-stash/file_${i}"
            rm "$f"
            i=$((i+1))
          done

      - name: Terraform init (local backend)
        run: terraform init -reconfigure -input=false

      - name: Detect existing S3 bucket & DynamoDB table
        shell: bash
        run: |
          set -euo pipefail

          PREFIX="${TF_VAR_bucket_prefix_name:-tf-state-s3-bucket}"
          SUFFIX="77g78ef5w"
          BUCKET="${PREFIX}-${SUFFIX}"

          # Buckets are global - list and match by name
          EXISTING_BUCKET="$(aws s3api list-buckets \
            --query "Buckets[?Name=='${BUCKET}'].Name | [0]" \
            --output text 2>/dev/null || true)"
          if [ "${EXISTING_BUCKET}" != "None" ] && [ -n "${EXISTING_BUCKET}" ]; then
            echo "Detected bucket: ${EXISTING_BUCKET}"
            echo "TF_VAR_create_bucket=false"                    >> "$GITHUB_ENV"
            echo "TF_VAR_existing_bucket_name=${EXISTING_BUCKET}" >> "$GITHUB_ENV"
          fi

          LOCK="${TF_VAR_lock_table:-tf-state-lock}"
          if aws dynamodb describe-table --table-name "${LOCK}" >/dev/null 2>&1; then
            echo "Detected lock table: ${LOCK}"
            echo "TF_VAR_create_lock_table=false"               >> "$GITHUB_ENV"
            echo "TF_VAR_existing_lock_table=${LOCK}"           >> "$GITHUB_ENV"
          fi
    
      # If these policies already exist, tell TF to skip creating them
      - name: Detect existing IAM policy ARNs (robust)
        shell: bash
        env:
          AWS_PAGER: ""           # avoid paging
          AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        run: |
          set -o pipefail
          set +e  # do not fail the step on aws/list errors

          get_arn () {
            local name="$1"
            # Use safe JMESPath and swallow errors
            local arn
            arn=$(aws iam list-policies --scope Local --no-paginate \
              --query "Policies[?PolicyName=='${name}'].Arn | [0]" \
              --output text 2>/dev/null)
            # Normalize to empty
            if [[ -z "$arn" || "$arn" == "None" || "$arn" == "null" ]]; then
              echo ""
            else
              echo "$arn"
            fi
          }

          B_ACK="$(get_arn tf-backend-rw)"
          VPC_APPLY="$(get_arn tf-vpc-apply)"

          # Export only when present
          if [[ -n "$B_ACK" ]]; then
            echo "TF_VAR_existing_backend_rw_policy_arn=$B_ACK" >> "$GITHUB_ENV"
          fi
          if [[ -n "$VPC_APPLY" ]]; then
            echo "TF_VAR_existing_vpc_apply_policy_arn=$VPC_APPLY" >> "$GITHUB_ENV"
          fi

          echo "Detected tf-backend-rw: ${B_ACK:-<none>}"
          echo "Detected tf-vpc-apply : ${VPC_APPLY:-<none>}"

          # Always succeed; later TF will decide to (re)create if these are empty
          exit 0
    
      - name: Debug TF vars for S3/DDB
        run: |
          echo "TF_VAR_create_bucket=${TF_VAR_create_bucket:-<unset>}"
          echo "TF_VAR_existing_bucket_name=${TF_VAR_existing_bucket_name:-<unset>}"
          echo "TF_VAR_create_lock_table=${TF_VAR_create_lock_table:-<unset>}"
          echo "TF_VAR_existing_lock_table=${TF_VAR_existing_lock_table:-<unset>}"

      - name: Debug TF vars for IAM policies
        run: |
          echo "TF_VAR_existing_backend_rw_policy_arn=${TF_VAR_existing_backend_rw_policy_arn:-<unset>}"
          echo "TF_VAR_existing_vpc_apply_policy_arn=${TF_VAR_existing_vpc_apply_policy_arn:-<unset>}"

      - name: Debug OIDC TF vars
        run: |
          echo "TF_VAR_create_oidc_provider=${TF_VAR_create_oidc_provider:-<unset>}"
          echo "TF_VAR_oidc_provider_arn=${TF_VAR_oidc_provider_arn:-<unset>}"
          echo "TF_VAR_create_oidc_role=${TF_VAR_create_oidc_role:-<unset>}"

      - name: Plan targeted modules
        run: |
          terraform plan -target=module.s3_bucket_state_oidc -no-color
          terraform plan -target=module.iam_tf_policies     -no-color
          terraform plan -target=module.github_oidc         -no-color    

      # If a GitHub OIDC provider/role already exists, don’t recreate them
      - name: Detect existing GitHub OIDC provider/role
        id: detect_oidc
        shell: bash
        env:
          AWS_PAGER: ""
        run: |
          set -euo pipefail

          WANT_HOST="token.actions.githubusercontent.com"  # normalized target
          PROV_ARN=""
          ARNS=$(aws iam list-open-id-connect-providers \
            --query 'OpenIDConnectProviderList[].Arn' --output text 2>/dev/null || true)

          for A in $ARNS; do
            URL=$(aws iam get-open-id-connect-provider \
            --open-id-connect-provider-arn "$A" \
            --query 'Url' --output text 2>/dev/null || true)
          # normalize what AWS returns
            URL=${URL#https://}
            URL=${URL#http://}
            if [ "$URL" = "$WANT_HOST" ]; then
              PROV_ARN="$A"
              break
            fi
          done

          ROLE_ARN=$(aws iam get-role --role-name github-oidc-provider-aws \
            --query 'Role.Arn' --output text 2>/dev/null || true)
          [ "$ROLE_ARN" = "None" ] && ROLE_ARN=""

          # Export variables that the ROOT module actually consumes
          if [ -n "$PROV_ARN" ]; then
            echo "TF_VAR_create_oidc_provider=false"  >> "$GITHUB_ENV"
            echo "TF_VAR_oidc_provider_arn=$PROV_ARN" >> "$GITHUB_ENV"
          fi
          if [ -n "$ROLE_ARN" ]; then
            echo "TF_VAR_create_oidc_role=false" >> "$GITHUB_ENV"
          fi

          echo "PROV_ARN=$PROV_ARN" >> "$GITHUB_OUTPUT"
          echo "ROLE_ARN=$ROLE_ARN" >> "$GITHUB_OUTPUT"
          
          
      # Create/ensure: S3+DDB backend, IAM policies (or reuse), OIDC provider+role (or reuse)
      - name: Terraform apply (bootstrap modules only)
        run: |
          set -euo pipefail
          terraform apply \
            -input=false \
            -target=module.s3_bucket_state_oidc \
            -target=module.iam_tf_policies \
            -target=module.github_oidc \
            -auto-approve

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      # Export outputs for the deploy job (prefer “effective” outputs)
      - name: Export bootstrap outputs
        id: export_bootstrap
        shell: bash
        run: |
          set -euo pipefail
          tfjson=$(terraform output -json || echo '{}')

          BUCKET=$(echo "$tfjson" | jq -r '.s3_bucket_id_output.value // .s3_bucket_id.value // empty')
          TABLE=$(echo "$tfjson"  | jq -r '.lock_table_name_output.value // .lock_table_name.value // empty')
          KEY=$(echo "$tfjson"    | jq -r '.backend_key.value // empty')

          ROLE_ARN_TF=$(echo "$tfjson" | jq -r '.effective_role_arn.value // empty')
          OIDC_ARN_TF=$(echo "$tfjson" | jq -r '.effective_oidc_provider_arn.value // empty')
          BACKEND_POL=$(echo "$tfjson" | jq -r '.tf_backend_rw_policy_arn.value // .backend_policy_arn.value // empty')
          VPC_APPLY_POL=$(echo "$tfjson" | jq -r '.tf_vpc_apply_policy_arn.value // .vpc_apply_policy_arn.value // empty')

          [ -z "$KEY" ] && KEY="tfstate/terraform.tfstate"
          [ -z "$ROLE_ARN_TF" ] && ROLE_ARN_TF="${{ steps.detect_oidc.outputs.ROLE_ARN }}"
          [ -z "$OIDC_ARN_TF" ] && OIDC_ARN_TF="${{ steps.detect_oidc.outputs.PROV_ARN }}"

          echo "bucket=$BUCKET"                 >> $GITHUB_OUTPUT
          echo "table=$TABLE"                   >> $GITHUB_OUTPUT
          echo "key=$KEY"                       >> $GITHUB_OUTPUT
          echo "role_arn=$ROLE_ARN_TF"          >> $GITHUB_OUTPUT
          echo "arn_oidc=$OIDC_ARN_TF"          >> $GITHUB_OUTPUT
          echo "backend_policy_arn=$BACKEND_POL" >> $GITHUB_OUTPUT
          echo "vpc_apply_policy_arn=$VPC_APPLY_POL" >> $GITHUB_OUTPUT

      - name: Restore S3 backend files
        run: |
          set -euo pipefail
          if [ ! -f .ci-backend-stash/manifest.txt ]; then
            exit 0
          fi
          i=0
          while IFS= read -r dest; do
            mkdir -p "$(dirname "$dest")"
            mv ".ci-backend-stash/file_${i}" "$dest"
            i=$((i+1))
          done < .ci-backend-stash/manifest.txt
          rm -rf .ci-backend-stash

    outputs:
      bucket:               ${{ steps.export_bootstrap.outputs.bucket }}
      table:                ${{ steps.export_bootstrap.outputs.table }}
      key:                  ${{ steps.export_bootstrap.outputs.key }}
      role_arn:             ${{ steps.export_bootstrap.outputs.role_arn }}
      arn_oidc:             ${{ steps.export_bootstrap.outputs.arn_oidc }}
      backend_policy_arn:   ${{ steps.export_bootstrap.outputs.backend_policy_arn }}
      vpc_apply_policy_arn: ${{ steps.export_bootstrap.outputs.vpc_apply_policy_arn }}  

  deploy:
    name: Backend migrate ➜ Plan / Apply (OIDC)
    runs-on: ubuntu-latest
    needs: bootstrap

    if: ${{ needs.bootstrap.result == 'success' && needs.bootstrap.outputs.role_arn != '' && needs.bootstrap.outputs.bucket != '' && needs.bootstrap.outputs.key != '' }}  

    env:
      TF_BACKEND_BUCKET:       ${{ needs.bootstrap.outputs.bucket }}
      TF_BACKEND_KEY:          ${{ needs.bootstrap.outputs.key }}
      TF_BACKEND_DDB_TABLE:    ${{ needs.bootstrap.outputs.table }}
      TF_VAR_oidc_provider_arn: ${{ needs.bootstrap.outputs.arn_oidc }} 
      AWS_OIDC_ROLE_ARN:       ${{ needs.bootstrap.outputs.role_arn }}

      TF_VAR_existing_backend_rw_policy_arn: ${{ needs.bootstrap.outputs.backend_policy_arn }}
      TF_VAR_existing_vpc_apply_policy_arn:  ${{ needs.bootstrap.outputs.vpc_apply_policy_arn }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ env.AWS_OIDC_ROLE_ARN }}
          role-session-name: gha-terraform

      - name: Verify caller identity (OIDC)
        run: aws sts get-caller-identity

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.3
          terraform_wrapper: false

      - name: Cache .terraform providers
        uses: actions/cache@v4
        with:
          path: |
            **/.terraform
            ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-tf-${{ hashFiles('**/.terraform.lock.hcl') }}
          restore-keys: |
            ${{ runner.os }}-tf-

      - name: Echo role ARN for aws oidc github role
        run: |
          echo "Assuming role: $AWS_OIDC_ROLE_ARN"           
      
      - name: Terraform fmt (non-blocking)
        run: terraform fmt -recursive -diff || true

      # flip creation flags for second (and future) runs
      - name: Set TF vars for existing state resources
        run: |
          echo "TF_VAR_create_bucket=false"                         >> $GITHUB_ENV
          echo "TF_VAR_create_lock_table=false"                     >> $GITHUB_ENV
          echo "TF_VAR_existing_bucket_name=${TF_BACKEND_BUCKET}"   >> $GITHUB_ENV
          echo "TF_VAR_existing_lock_table=false" >> $GITHUB_ENV

      - name: Terraform init (S3 backend, migrate state)
        shell: bash
        run: |
          set -euo pipefail
          ARGS=(-migrate-state -input=false)

          if [ -n "${TF_BACKEND_BUCKET:-}" ]; then
            ARGS+=(-backend-config="bucket=${TF_BACKEND_BUCKET}")
          fi
          if [ -n "${TF_BACKEND_KEY:-}" ]; then
            ARGS+=(-backend-config="key=${TF_BACKEND_KEY}")
          fi
          if [ -n "${AWS_REGION:-}" ]; then
            ARGS+=(-backend-config="region=${AWS_REGION}")
          fi
          # Optional: only include dynamodb_table if you actually use it
          if [ -n "${TF_BACKEND_DDB_TABLE:-}" ]; then
            ARGS+=(-backend-config="use_lockfile=false")
          fi
          # Do NOT pass use_lockfile; it isn't an S3 backend setting and if empty it breaks init
          terraform init "${ARGS[@]}"        

      - name: Terraform validate
        run: terraform validate -no-color

      # PRs: plan only
      - name: Terraform plan (PR)
        if: github.event_name == 'pull_request'
        run: |
          set -euo pipefail
          terraform plan -input=false \             
            -target=module.s3_bucket_state_oidc \
            -target=module.iam_tf_policies \
            -no-color \ 
            -out=tfplan.bin

      - name: Upload plan (PR)
        if: github.event_name == 'pull_request'
        uses: actions/upload-artifact@v4
        with:
          name: tfplan
          path: tfplan.bin
          retention-days: 3


      - name: Terraform apply
        if: github.event_name != 'pull_request' && (github.event_name != 'workflow_dispatch' || inputs.do_apply == 'true')
        run: |
          set -euo pipefail
          terraform apply -input=false -target=module.s3_bucket_state_oidc -target=module.iam_tf_policies -auto-approve

      # Manual destroy only
      - name: Terraform destroy (manual only)
        if: github.event_name == 'workflow_dispatch' && inputs.do_destroy == 'true'
        run: |
          set -euo pipefail 
          terraform destroy -input=false -target=module.s3_bucket_state_oidc -target=module.iam_tf_policies -auto-approve

